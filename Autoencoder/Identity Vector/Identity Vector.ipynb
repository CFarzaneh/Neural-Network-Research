{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the behavior of Linear Activation Functions in a Simple Autoencoder\n",
    "Cameron Farzaneh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The goal of this project is to gain insight as to why a Linear Activation Function is not able to reconstruct my input data, and why it is behaving the way it is when reducing the dimensions from two, down to one in latency space. The purpose of this experiment is to gain futurer insight into Autoencoders, the basic structure of Neural Networks, and to gain a deeper understanding into the Mathematics involved during the entire process.\n",
    "\n",
    "In this experiment, I was not able to successfully reconstruct the Input Data. My goal is to understand why this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of vectors with magnitudes between -3 and 3. These vectors are unit vectors 45 degrees from the X and Y axis, and 90 degrees from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the dataset looks like:\n",
    "![title](img/dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, to construct this dataset, we are first creating basis vectors. These are unit vectors so we can easily control the magnitude. Our basis vectors, U1 and U2 are:\n",
    "$$U_1 = <\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}>$$\n",
    "$$U_2 = <-\\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}}>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we can multiply the basis vectors by magnitudes randomly picked between -3 and 3. Doing this, we can construct the dataset above. Our dataset size is 10,000. We can simply store this in a NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the Autoencoder looks like:\n",
    "![title](img/network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this diagram, $W_1$ and $W_2$ are both weights. They are initialized randomly. $B_z$, $B_1$, and $B_2$ are our biases. $X_1$ and $X_2$ are our output neurons. The weights $W_1$ and $W_2$ are shared, however, they are transposed in-between Z and the reconstruction layer. All together, there are 3 biases and 2 weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This autoencoder has one neuron in the hidden layer and two neurons representing for both the input and output layers. The goal of this autoencoder is to reduce the dimensionality from two (the dataset) into one dimension in latency space, and reconstruct the same vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder works by taking in two inputs, $X_1$ and $X_2$. $X_1$ and $X_2$ represent the X and Y componants of a single vector (either Purple or Yellow).\n",
    "So $X_1$ could be the Y compontant and $X_2$ could be the X compontent (or Vice Versa).  Because our autoencoder has only one node in the middle, the transformation from the two nodes to Z is simply a dot product. \n",
    "\n",
    "**Note. This is only the case because we are reducing from two neurons to one! Typically, this step would be matrix multiplcation.**\n",
    "\n",
    "Our forumula for Z is equal to:\n",
    "$$Z = \\sum\\limits_{i=1}^{2}{X_iW_i} + B_z$$\n",
    "\n",
    "<center>or</center>\n",
    "\n",
    "$$Z = X_1W_1+X_2W_2+B_z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must look at our possibilities as inputs for $X_1$ and $X_2$.\n",
    "If the input is a point on the purple line, then $X_1$ and $X_2$ would either both be positive, or both be negative.\n",
    "Similarly, if the input is a point on the yellow line, then $X_1$ is either negative and $X_2$ is positive, or $X_1$ is positive and $X_2$ is negative.\n",
    "\n",
    "We can write this as:\n",
    "$$\\frac{a}{\\sqrt{2}}<-1,1>$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of this, our Z function will look different depending on the input point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the state in which the autoencoder was built, it was not able to successfully reconstruct both vectors. As you can see in the diagram below, only one line was successfully being successfully reconstructed. This must mean that the Autoencoder was only able to learn one of the vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To optimize the cost function, Adam Optimizer was the fastest in comparison to Gradient Decent and Adagrad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/results/result1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In latency space, it is clear that the input data for the purple line was successfully being transformed into one-dimension. This is not the case for the yellow line. All the points appear to be cluttered around the point 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between the points in latency space should correspond to the distance in the Input data. This is why the purple line, in latency space, looks almost identitcal to the input data and reconstrcution. The purple line appeared to be successfully keeping the same distance between the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why is the yellow line being mapped to only 0? Why isn't the autoencoder able to learn both lines, and maintain the distance apart in latency space for both lines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
